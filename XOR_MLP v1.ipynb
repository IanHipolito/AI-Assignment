{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/11/2019\n",
    "\n",
    "The code here is close to Nielsen. Each activation is treated as a column vector, even the last one which for XOR is just a simple number and is encloded in a shape (1,1) column vector of just one row, i.e if activation value of output neuron is a, then it is computed as np.array([[a]]).\n",
    "\n",
    "Can easily adapt code here for the MLP excercises and the Iris classification problem.\n",
    "But you may need to use more than 2 hidden neurons and more than 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return  1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a*(1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(2,2)\n",
    "        self.b2 = np.random.randn(2,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,2)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = XOR_MLP()\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 3.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "class XOR_MLP_COPY:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(3,2)\n",
    "        self.b2 = np.random.randn(3,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,3)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor2 = XOR_MLP_COPY()\n",
    "xs = xor2.train_inputs.T\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "epochs = 1000\n",
    "c = xor2.train(epochs, 3.0)\n",
    "print(xor2.feedforward(xs))\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more general purpose MLP with m input neurons, n hidden neurons and o output neurond\n",
    "# You must complete this code yourself\n",
    "class MLP:\n",
    "    def __init__(self, m, n, o):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of N neurons\n",
    "        self.w2 = np.random.randn(n,m)\n",
    "        self.b2 = np.random.randn(n,1)\n",
    "        \n",
    "        # output layer has O neurons but code is incorrect\n",
    "        # code here needs to be modified\n",
    "        self.w3 = np.random.randn(o,n)\n",
    "        self.b3 = np.random.randn(o,1)\n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)   \n",
    "        # Format the output for better readability\n",
    "        formatted_output = self.format_output(a3s)\n",
    "        return formatted_output         \n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs,ys):            \n",
    "            a1 = x.reshape(self.m, 1)        # convert input vector x into (2,1) column vector\n",
    "            y = y.reshape(self.o, 1)         # convert output vector y into (1,1) column vector\n",
    "\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            a2 = a2.reshape(self.n, 1)       # convert a2 into (2,1) column vector\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            a3 = a3.reshape(self.o, 1)       # convert a3 into (1,1) column vector\n",
    "\n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(self.train_inputs ,self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        plt.title('Loss Per Epochs/Iteration')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        return cost\n",
    "                \n",
    "    def format_output(self, output):\n",
    "        formatted_output = \"\\n\".join(\n",
    "            \", \".join(f\"{value:.4f}\" for value in row) for row in output.T\n",
    "        )\n",
    "        return formatted_output\n",
    "                \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - Cross Entropy Cost Version\n",
    "class MLP:\n",
    "    def __init__(self, m, n, o):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of N neurons\n",
    "        self.w2 = np.random.randn(n,m)\n",
    "        self.b2 = np.random.randn(n,1)\n",
    "        \n",
    "        # output layer has O neurons but code is incorrect\n",
    "        # code here needs to be modified\n",
    "        self.w3 = np.random.randn(o,n)\n",
    "        self.b3 = np.random.randn(o,1)\n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)   \n",
    "        # Format the output for better readability\n",
    "        formatted_output = self.format_output(a3s)\n",
    "        return formatted_output         \n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs,ys):            \n",
    "            a1 = x.reshape(self.m, 1)        # convert input vector x into (2,1) column vector\n",
    "            y = y.reshape(self.o, 1)         # convert output vector y into (1,1) column vector\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            a2 = a2.reshape(self.n, 1)       # convert a2 into (2,1) column vector\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            a3 = a3.reshape(self.o, 1)       # convert a3 into (1,1) column vector\n",
    "\n",
    "            delta3 = (a3-y)               # Cross Entropy Cost    \n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(self.train_inputs ,self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        plt.title('Loss Per Epochs/Iteration')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        return cost\n",
    "                \n",
    "    def format_output(self, output):\n",
    "        formatted_output = \"\\n\".join(\n",
    "            \", \".join(f\"{value:.4f}\" for value in row) for row in output.T\n",
    "        )\n",
    "        return formatted_output\n",
    "                \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the outputs of these correct?\n",
    "p1 = MLP(3,4,2)\n",
    "print('\\n W2 = \\n',p1.w2, '\\n W3 = \\n', p1.w3, '\\n')\n",
    "\n",
    "p2 = MLP(4,6,3)\n",
    "print('\\n W2 = \\n', p2.w2, '\\nW3 = \\n', p2.w3, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1 : Testing MLP\n",
    "p1_mlp = MLP(3,4,1)\n",
    "\n",
    "p1_mlp.train_inputs = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "p1_mlp.train_outputs = np.array([0,1,1,0])\n",
    "\n",
    "xs = p1_mlp.train_inputs.T\n",
    "\n",
    "print(\"\\nBefore Training:\\n\" + p1_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "# The higher the learning rate the more unstale the grapgh is\n",
    "#learning_rate = 10.0\n",
    "learning_rate = 50.0\n",
    "\n",
    "c = p1_mlp.train(epochs, learning_rate)\n",
    "\n",
    "print(\"\\nAfter Training:\\n\" + p1_mlp.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2 - Neural Network with 3 input and 2 output\n",
    "p2_mlp = MLP(3,4,2)\n",
    "\n",
    "p2_mlp.train_inputs = np.array([[1,1,0],\n",
    "                                [1,-1,-1],\n",
    "                                [-1,1,1],\n",
    "                                [-1,-1,1],\n",
    "                                [0,1,-1],\n",
    "                                [0,-1,-1],\n",
    "                                [1,1,1]])\n",
    "\n",
    "p2_mlp.train_outputs = np.array([[1,0],\n",
    "                                 [0,1],\n",
    "                                 [1,1],\n",
    "                                 [1,0],\n",
    "                                 [1,0],\n",
    "                                 [1,1],\n",
    "                                 [1,1]])\n",
    "\n",
    "xs = p2_mlp.train_inputs.T\n",
    "print(\"\\nBefore Training:\\n\" + p2_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 2000\n",
    "learning_rate = 10.0\n",
    "\n",
    "c = p2_mlp.train(epochs, learning_rate)\n",
    "print(\"\\nAfter Training:\\n\" + p2_mlp.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3 - Transportation Mode Choice\n",
    "# Possible Outputs: Bus [1,0,0] │ Train [0,1,0] │ Car [0,0,1]\n",
    "# Gender: 0 = Male │ 1 = Female\n",
    "# Car Ownership: 0 │ 1 │ 2 \n",
    "# Travel Costs: 0 = Cheap │ 1 = Standard │ 2 = Expensive\n",
    "# Income: 0 = Low │ 1 = Medium │ 2 = High\n",
    "\n",
    "p3_mlp = MLP(4,6,3)\n",
    "\n",
    "p3_mlp.train_inputs = np.array([[0,0,0,0], \n",
    "                                [0,1,0,1], \n",
    "                                [1,1,0,1], \n",
    "                                [1,0,0,0], \n",
    "                                [0,1,0,1], \n",
    "                                [0,0,1,1], \n",
    "                                [1,1,1,1], \n",
    "                                [1,1,2,2], \n",
    "                                [0,2,2,1], \n",
    "                                [1,2,2,2]])\n",
    "\n",
    "p3_mlp.train_outputs = np.array([[1,0,0], \n",
    "                                 [1,0,0], \n",
    "                                 [0,1,0], \n",
    "                                 [1,0,0], \n",
    "                                 [1,0,0], \n",
    "                                 [0,1,0], \n",
    "                                 [0,1,0], \n",
    "                                 [0,0,1], \n",
    "                                 [0,0,1], \n",
    "                                 [0,0,1]])\n",
    "\n",
    "xs = p3_mlp.train_inputs.T\n",
    "print(\"\\nBefore Training:\\n\" + p3_mlp.feedforward(xs))\n",
    "print(\"cost = \", str(c[-1]))\n",
    "\n",
    "epochs = 2000\n",
    "learning_rate = 10.0\n",
    "\n",
    "c = p3_mlp.train(epochs, learning_rate)\n",
    "print(\"\\nAfter Training:\\n\" + p3_mlp.feedforward(xs))\n",
    "print(\"cost = \", str(c[-1]))\n",
    "\n",
    "# Female Test Case: Car Ownership, Standard Travel Costs, medium Income\n",
    "print(\"\\nWomen Test Case:\")\n",
    "test_case = np.array([1, 1, 1, 1]).reshape(4,1)\n",
    "print(p3_mlp.predict(test_case))\n",
    "\n",
    "# Copying data to dataframe and saving it to a csv file seperated by commas\n",
    "p3_mlp_df = pd.DataFrame(p3_mlp.train_inputs)\n",
    "p3_mlp_df.to_csv('transport.csv', sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and reading iris_data.csv into a dataframe\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "# Last column is the output\n",
    "training_outputs = df.iloc[:,-1]\n",
    "\n",
    "# Converting the output to a one hot encoded vector\n",
    "training_outputs = pd.get_dummies(training_outputs)\n",
    "\n",
    "# Converting the dataframe to a numpy array\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "\n",
    "# Convert y true and false to 1 and 0\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "# Drop last column from dataframe\n",
    "df = df.drop(df.columns[[-1]], axis=1)\n",
    "\n",
    "# Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "# Clean up data\n",
    "training_inputs = training_inputs.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mlp = MLP(4,7,3)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "xs = training_inputs.T\n",
    "print(\"\\nOutputs Before Training:\\n\" + iris_mlp.feedforward(xs))\n",
    "print(\"cost = \", str(c[-1]))\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.8\n",
    "\n",
    "c = iris_mlp.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(\"\\nOutputs after training:\\n\" + iris_mlp.feedforward(xs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
